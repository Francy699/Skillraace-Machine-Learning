{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOltn5Qg7Ywkpnsmio6yfSi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**BACKPROPAGATION**\n","Backpropagation is an algorithm used to train artificial neural networks. It works by computing the gradient of the loss function with respect to the weights of the network."],"metadata":{"id":"KtS_xyikiVDn"}},{"cell_type":"code","source":["#Import necessary libraries\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"Mp_hTodLipla","executionInfo":{"status":"ok","timestamp":1719036415046,"user_tz":-330,"elapsed":2181,"user":{"displayName":"Francy Pothuraju","userId":"04225123381959978702"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#sigmoid activation funtion\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))"],"metadata":{"id":"Z-S6OwPBjhft","executionInfo":{"status":"ok","timestamp":1719036457279,"user_tz":-330,"elapsed":503,"user":{"displayName":"Francy Pothuraju","userId":"04225123381959978702"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#derivative of the sigmoid funtion\n","def sigmoid_derivative(x):\n","    return x*(1-x)"],"metadata":{"id":"dnf2eXLcjutu","executionInfo":{"status":"ok","timestamp":1719036518049,"user_tz":-330,"elapsed":839,"user":{"displayName":"Francy Pothuraju","userId":"04225123381959978702"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#neural network class\n","class NeuralNetwork:\n","  def __init__(self,input_size,hidden_size,output_size):\n","    self.input_size=input_size\n","    self.hidden_size=hidden_size\n","    self.output_size=output_size\n","\n","    #initialize weights\n","    self.weights_input_hidden=np.random.randn(self.input_size,self.hidden_size)\n","    self.weights_hidden_output=np.random.randn(self.hidden_size,self.output_size)\n","\n","    #initialize the bias\n","    self.bias_hidden=np.zeros((1,self.hidden_size))\n","    self.bias_output=np.zeros((1,self.output_size))\n","\n","  #forward propagation\n","  def forward_propagation(self,x):\n","    self.hidden_input=np.dot(x,self.weights_input_hidden)+self.bias_hidden\n","    self.hidden_output=sigmoid(self.hidden_input)\n","    self.final_input=np.dot(self.hidden_output,self.weights_hidden_output)+self.bias_output\n","    self.final_output=sigmoid(self.final_input)\n","    return self.final_output\n","\n","  #backward propagation\n","  def backward_propagation(self,x,y,output,learning_rate):\n","    #calculate error\n","    output_error=y-output\n","    output_delta=output_error*sigmoid_derivative(output)\n","\n","    hidden_error=output_delta.dot(self.weights_hidden_output.T)\n","    hidden_delta=hidden_error*sigmoid_derivative(self.hidden_output)\n","\n","    #update weights and biases\n","    self.weights_hidden_output+=self.hidden_output.T.dot(output_delta)*learning_rate\n","    self.bias_output+=np.sum(output_delta,axis=0,keepdims=True)*learning_rate\n","    self.weights_input_hidden+=x.T.dot(hidden_delta)*learning_rate\n","    self.bias_hidden+=np.sum(hidden_delta,axis=0,keepdims=True)*learning_rate\n","\n","  #train the model\n","  def train(self,x,y,learning_rate,epochs):\n","    for epoch in range(epochs):\n","      output=self.forward_propagation(x)\n","      self.backward_propagation(x,y,output,learning_rate)\n","      if (epoch+1)%1000==0:\n","        loss=np.mean(np.square(y-output))\n","        print(f\"Epoch: {epoch+1}, Loss: {loss}\")\n","\n","#load the iris dataset\n","iris=load_iris()\n","X=iris.data\n","y=iris.target\n","\n","  # Convert the target to one-hot encoding\n","y = np.eye(len(np.unique(y)))[y]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Initialize and train the neural network\n","input_size = X_train.shape[1]\n","hidden_size = 5\n","output_size = y_train.shape[1]\n","\n","nn = NeuralNetwork(input_size, hidden_size, output_size)\n","nn.train(X_train, y_train, epochs=10000, learning_rate=0.01)\n","\n","# Make predictions on the test set\n","predictions = nn.forward_propagation(X_test)\n","predictions = np.argmax(predictions, axis=1)\n","y_test_labels = np.argmax(y_test, axis=1)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test_labels, predictions)\n","print(f\"Test set accuracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQEH8KaAj9eR","executionInfo":{"status":"ok","timestamp":1719040428485,"user_tz":-330,"elapsed":1311,"user":{"displayName":"Francy Pothuraju","userId":"04225123381959978702"}},"outputId":"141691c2-c67b-452a-c676-149ed7458b27"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1000, Loss: 0.017042711164388255\n","Epoch: 2000, Loss: 0.012602216669131333\n","Epoch: 3000, Loss: 0.011404649140443484\n","Epoch: 4000, Loss: 0.010878628464532343\n","Epoch: 5000, Loss: 0.010587405757821895\n","Epoch: 6000, Loss: 0.010401617001974131\n","Epoch: 7000, Loss: 0.010270204342640041\n","Epoch: 8000, Loss: 0.010168920379878533\n","Epoch: 9000, Loss: 0.010084685566543211\n","Epoch: 10000, Loss: 0.010009708861690444\n","Test set accuracy: 1.0\n"]}]}]}